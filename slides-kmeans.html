<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>K-Means Clustering in R</title>
    <meta charset="utf-8" />
    <meta name="author" content="Uli Niemann" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="assets\css\my-theme.css" type="text/css" />
    <link rel="stylesheet" href="assets\css\my-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: title-slide, center, bottom

# K-Means Clustering in R

## RStudio Instructor Certification Teaching Exam

### Uli Niemann

### 2020/08/21

#### [https://github.com/unmnn/sample-lesson-kmeans/](https://github.com/unmnn/sample-lesson-kmeans/)

.courtesy[&amp;#x1F4F7; Photo courtesy of Ulrich Arendt]

???

Today, we will learn about unsupervised learning.

---

## Recap: Supervised Learning

**General idea**: train a model which estimates the value of the response based on the values of the predictors. 

`$$\hat{Y}=f(\mathbf{X})$$`




&lt;img src="slides-kmeans_files/figure-html/reg-model-residuals-1.png" width="708.661417322835" style="display: block; margin: auto;" /&gt;

???

As a quick recap, so far in this course, we have exclusively discussed supervised models.

The general idea of supervised learning is to train a function f which estimates the value of the response Y based on the values of the predictors X.

An example is predicting the sale price of a house based on the size of its living area.
Here, house price is the response, living area is the sole predictor and the blue regression line is our model. 
Usually, we have more than just 1 predictor, e.g., number of rooms or crime rate of the town.

To create this regression line or any supervised model, we need observations for which we know the true value of the response. 
Once the model is trained, we can use it to predict the response values for observation for which we don't know the true value of the response. 
We can also use these true values to evaluate how well the predictions of our model are.
For example, the red dashed lines represent the so called residuals, the differences between the predicted and true values of the response. We can use the residuals to estimate the generalization error of the model. 

---

## Unsupervised Learning

**Goal**: look for interesting patterns in the data without having a response

&lt;img src="images/clustering-examples.png" width="90%" style="display: block; margin: auto;" /&gt;

.footnote[Photos: left: [Lucrezia Carnelos](https://unsplash.com/photos/wQ9VuP_Njr4); center: [National Cancer Institute](https://unsplash.com/photos/sIqWYiNLiJU); right: [Agence Olloweb](https://unsplash.com/photos/qfp4-Ud6Fyg)]

???

Now, for many tasks, we don't have a response, but we are still interested in finding patterns or structure in the data.

This scenario is known as unsupervised learning.

A company might want to increase the response rate of their marketing campaigns. 
They are interested in finding customer subgroups with similar demographics and preferences. 
The company can fit their ads to the characteristics and preferences of each respective subgroup. 
For example, ads for vegan products might work better in bigger cities in comparison to rural areas and focus their marketing budget to this customer subgroup.

Another example would be finding subtypes/phenotypes of disease like breast cancer, e.g. using gene expression measurements.

Or a company that runs a webshop might want to discover typical usage patterns on their website. 
Maybe they identify 2 distinct types of webshop users. 
They could use insights on this analysis to improve UI, maybe provide user-type personalized offers and thus, increase conversion rate.

--

.pull-left70[

**Clustering**: methods to find homogeneous subgroups in a dataset

Properties of a good clustering results:

- objects within a cluster are as **similar** as possible
- objects from different clusters are as **dissimilar** as possible

]

.pull-right30[

&amp;nbsp;

&amp;nbsp;

&amp;rarr; require: **similarity measure**

]

???

These three scenarios are examples of data clustering. 

Clustering refers to a set of methods to find homogeneous subgroups in a dataset.

A good clustering result groups similar objects into the same cluster and dissimilar objects into different clusters.

To determine which objects are similar and which are not similar, we need a similarity or distance measure.

---

## K-Means Clustering

.pull-left60[

**General idea**: partition the observations into a pre-specified number of subgroups (clusters)

**Centroid**: mean vector of a cluster's observations

**Objective**: minimize total intra-cluster variation

`$$\min_{C_1,C_2,\ldots,C_K}\left\{ \sum_{k} W(C_k) \right\}$$`

Squared **Euclidean distance** is often used to measure the **within-cluster variation** `\(W\)` of a cluster `\(C_k\)`:

`$$W(C_k)=\sum_{i\in C_k}\sum_j^p(x_{ij}-\bar{x}_{kj})^2$$`

where 

- `\(i\)` is the index of an observation assigned to `\(C_k\)`
- `\(\bar{x}_k\)` is the centroid of `\(C_k\)`

]

.pull-right40[



&amp;nbsp;

&lt;img src="slides-kmeans_files/figure-html/kmeans-1-1.png" width="396.850393700787" /&gt;

]

???

K-Means is probably the most popular clustering algorithm, this is why we learn it first.

The general idea is to partition the observations of our dataset into a pre-specified number of subgroups, the clusters. 

Each cluster is represented by a so called centroid, which is the average of the coordinates of the observations assigned to the cluster.

Now we want to find a set of clusters such that our objective function, the total intra-cluster variation W, is minimal.

What is intra-cluster variation?

The intra-cluster variation of a cluster is the sum of squared distances between the observations in the cluster and their centroid.

---

## K-Means Algorithm


```markdown
1. Select K initial centroids.
2. Repeat until the cluster assignments do not change anymore:  
    a. Assign each observation to the closest centroid.
    b. Recompute the positions of all centroids.
```

???

Okay, but how does the algorithm actually work? 

First, we select K initial centroids. 
Usually, we take the coordinates of randomly drawn observations from our data.

Then, we run this iterative step until the cluster assignments do not change anymore.

We assign each observations to the closest centroid and recompute the positions of every centroid.

---

class: middle 

## Example: K=3


```markdown
1. Select K initial centroids.
2. Repeat until the cluster assignments do not change anymore:  
    a. Assign each observation to the closest centroid.
    b. Recompute the positions of all centroids.
```

&lt;img src="slides-kmeans_files/figure-html/ex-1-1.png" width="453.543307086614" style="display: block; margin: auto;" /&gt;

???

As a practical example, let's apply K-Means with 3 clusters to this 2d dataset. 

---

class: middle 


```markdown
*1. Select K initial centroids.
2. Repeat until the cluster assignments do not change anymore:  
    a. Assign each observation to the closest centroid.
    b. Recompute the positions of all centroids.
```

&lt;img src="slides-kmeans_files/figure-html/ex-2-1.png" width="453.543307086614" style="display: block; margin: auto;" /&gt;

???

First, we pick 3 random observations from the data as our inital centroids (diamonds).

---

class: middle


```markdown
1. Select K initial centroids. 
2. Repeat until the cluster assignments do not change anymore:  
*   a. Assign each observation to the closest centroid.
    b. Recompute the positions of all centroids.
```

&lt;img src="slides-kmeans_files/figure-html/ex-3-1.png" width="453.543307086614" style="display: block; margin: auto;" /&gt;

???

Then, we assign each observation to the closest centroid. 
Similarity is measured by Euclidean distance.
We can see that each point is colored according to its closest centroid.

---

class: middle


```markdown
1. Select K initial centroids. 
2. Repeat until the cluster assignments do not change anymore:  
    a. Assign each observation to the closest centroid. 
*   b. Recompute the positions of all centroids.
```

&lt;img src="slides-kmeans_files/figure-html/ex-4-1.png" width="453.543307086614" style="display: block; margin: auto;" /&gt;

???

Now, we recompute the positions of each centroid.
The centroid is the mean vector over all observations assigned to the cluster.
The black diamond here is the old position of the centroid.

---

class: middle


```markdown
1. Select K initial centroids. 
2. Repeat until the cluster assignments do not change anymore:  
*   a. Assign each observation to the closest centroid.
    b. Recompute the positions of all centroids.
```

&lt;img src="slides-kmeans_files/figure-html/unnamed-chunk-3-1.png" width="453.543307086614" style="display: block; margin: auto;" /&gt;

???

We repeat the cluster assignment step. 
Since some of the observations previously assigned to the purple cluster now belong to the orange cluster, we again have to recompute the centroid positions. 


---

class: middle


```markdown
1. Select K initial centroids. 
2. Repeat until the cluster assignments do not change anymore:  
    a. Assign each observation to the closest centroid. 
*   b. Recompute the positions of all centroids.
```

&lt;img src="slides-kmeans_files/figure-html/unnamed-chunk-5-1.png" width="453.543307086614" style="display: block; margin: auto;" /&gt;

???

We repeat this process until the cluster memberships do not change anymore. 

---

class: middle


```markdown
1. Select K initial centroids. 
2. Repeat until the cluster assignments do not change anymore:  
*   a. Assign each observation to the closest centroid.
    b. Recompute the positions of all centroids.
```

&lt;img src="slides-kmeans_files/figure-html/unnamed-chunk-7-1.png" width="453.543307086614" style="display: block; margin: auto;" /&gt;

???

We do not stop here because the assignments have slightly changed.

---

class: middle


```markdown
1. Select K initial centroids. 
2. Repeat until the cluster assignments do not change anymore:  
    a. Assign each observation to the closest centroid. 
*   b. Recompute the positions of all centroids.
```

&lt;img src="slides-kmeans_files/figure-html/unnamed-chunk-9-1.png" width="453.543307086614" style="display: block; margin: auto;" /&gt;

---

class: middle


```markdown
1. Select K initial centroids. 
*2. Repeat until the cluster assignments do not change anymore:
    a. Assign each observation to the closest centroid. 
    b. Recompute the positions of all centroids. 
```

&lt;img src="slides-kmeans_files/figure-html/unnamed-chunk-10-1.png" width="453.543307086614" style="display: block; margin: auto;" /&gt;

???

This is our final result after 3 iterations.

---

exclude: true

## Properties of K-Means

K-means finds a **local** rather than a global optimum.

The clustering result will depend on the **initial (random) positions of the centroids**. 

&amp;xrarr; Run the algorithm **multiple times** from different initial configurations and select the result which yields minimum total intra-cluster variation.

.pull-left[

&lt;img src="slides-kmeans_files/figure-html/kmeans-local-optimum-1-1.png" width="311.811023622047" style="display: block; margin: auto;" /&gt;


]

.pull-right[



&lt;img src="slides-kmeans_files/figure-html/kmeans-local-optimum-2-1.png" width="311.811023622047" style="display: block; margin: auto;" /&gt;


]

???

Now although K-means is guaranteed to reduce the intra-cluster variation at each step, 
the algo finds a local rather than a global optimum.

We can see here that depending on the initial positions of the centroid, the algorithm might get "stuck" in a poor solution.

Hence, it is recommended to run the algorithm multiple times with different initial configurations and 
select the result which yields minimum total intra-cluster variation.

---

class: exercise-blue
name: formative-assessment-1

## Your Turn

#### Which of the plots below shows the correct result of K-means clustering with K=2? A, B or C?

&lt;img src="slides-kmeans_files/figure-html/yt-1-1.png" width="708.661417322835" style="display: block; margin: auto;" /&gt;

???

A: Centroid of a cluster is the mean of all points in the cluster. It is not the correct result, because the red diamond is outside of the cluster.  
B: K=3  
C: Correct.

// In K-means clustering, a centroid is the prototype of a cluster, i.e., the average (mean) of all the points in the cluster.
Which of the following plots shows the correct centroid positions after convergence?

---

## K-Means in `R`

The `kmeans()` function takes two mandatory arguments: 

* `x`: the data (data frame or numeric matrix)
* `centers`: number of clusters K or a set of initial centroids


???

Now, let's find out how to use K-means in R.

The algorithm is implemented in the `stats` package which is automatically attached to your `R` session, so we don't need to load any additional libraries. 

The kmeans() function has two mandatory arguments: 

- x: the data itself as matrix or data frame, and 
- centers, either the number of clusters or custom coordinates of the initial centroids

--

.pull-left60[

#### Example: Clustering Palmer Penguins Bill Sizes


```r
library(tidyverse)
(df &lt;- palmerpenguins::penguins %&gt;% 
  select(bill_length_mm, bill_depth_mm) %&gt;% 
  drop_na())
```

```
## # A tibble: 342 x 2
##    bill_length_mm bill_depth_mm
##             &lt;dbl&gt;         &lt;dbl&gt;
##  1           39.1          18.7
##  2           39.5          17.4
##  3           40.3          18  
##  4           36.7          19.3
##  5           39.3          20.6
##  6           38.9          17.8
##  7           39.2          19.6
##  8           34.1          18.1
##  9           42            20.2
## 10           37.8          17.1
## # ... with 332 more rows
```

]

.pull-right40[

&lt;img src="slides-kmeans_files/figure-html/palmer-initial-plot-1.png" width="396.850393700787" style="display: block; margin: auto;" /&gt;

]

???

Here, we apply K-Means on the Palmer penguins data to find subgroups of penguins based on the length and depth of their bills.

First, we select the two relevant variables and filter out incomplete rows.

---

## K-Means in `R`





```r
(clustering &lt;- kmeans(x = df, centers = 3))
```

```
## K-means clustering with 3 clusters of sizes 116, 141, 85
## 
## Cluster means:
##   bill_length_mm bill_depth_mm
## 1       45.51379      15.64397
## 2       38.40355      18.27943
## 3       50.90353      17.33647
## 
## Clustering vector:
##   [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
##  [43] 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2 1 2 2 2
##  [85] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 2 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2
## [127] 2 2 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 3 1 3 1 1 1 1 1 1 1 3 1 1 1 3 1
## [169] 3 1 3 3 1 1 1 1 1 1 1 1 3 1 1 1 3 3 3 1 1 1 3 1 3 1 3 3 1 1 3 1 1 1 1 1 3 1 1 1 1 1
## [211] 3 1 1 1 3 1 3 1 3 1 3 1 1 1 1 1 3 1 3 1 1 3 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 3 1 1 3 1
## [253] 3 1 3 1 1 1 3 1 1 3 3 1 3 1 3 1 3 1 1 3 1 3 1 3 3 1 3 1 1 3 1 3 1 3 1 3 1 3 3 3 1 3
## [295] 1 3 1 3 1 3 3 3 1 3 2 3 1 3 3 1 1 3 1 3 3 1 3 1 3 3 3 3 3 3 1 3 1 3 1 3 1 3 3 1 3 1
## [337] 1 3 1 3 3 3
## 
## Within cluster sum of squares by cluster:
## [1] 754.7437 944.4986 617.9859
##  (between_SS / total_SS =  79.8 %)
## 
## Available components:
## 
## [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
## [6] "betweenss"    "size"         "iter"         "ifault"
```

???

returns object of class "kmeans"

The print methods gives us already very useful diagnostics:

- size of the clusters
- centroid positions
- cluster assignment vector
- WSS for each cluster
- estimate of goodness of the clustering (the relative reduction in SS; the higher the better)
- names of available components

---

## Components of a `kmeans` object


```r
str(clustering)
```

```
## List of 9
##  $ cluster     : int [1:342] 2 2 2 2 2 2 2 2 2 2 ...
##  $ centers     : num [1:3, 1:2] 45.5 38.4 50.9 15.6 18.3 ...
##   ..- attr(*, "dimnames")=List of 2
##   .. ..$ : chr [1:3] "1" "2" "3"
##   .. ..$ : chr [1:2] "bill_length_mm" "bill_depth_mm"
##  $ totss       : num 11494
##  $ withinss    : num [1:3] 755 944 618
##  $ tot.withinss: num 2317
##  $ betweenss   : num 9177
##  $ size        : int [1:3] 116 141 85
##  $ iter        : int 2
##  $ ifault      : int 0
##  - attr(*, "class")= chr "kmeans"
```

???

We can use the `str()` function to see which components of the kmeans object are available:

- some sort of list with 9 elements

next slide

---

.font90[


```r
str(clustering)
```

```
## List of 9
*##  $ cluster     : int [1:342] 2 2 2 2 2 2 2 2 2 2 ...
*##  $ centers     : num [1:3, 1:2] 45.5 38.4 50.9 15.6 18.3 ...
##   ..- attr(*, "dimnames")=List of 2
##   .. ..$ : chr [1:3] "1" "2" "3"
##   .. ..$ : chr [1:2] "bill_length_mm" "bill_depth_mm"
##  $ totss       : num 11494
##  $ withinss    : num [1:3] 755 944 618
##  $ tot.withinss: num 2317
##  $ betweenss   : num 9177
##  $ size        : int [1:3] 116 141 85
##  $ iter        : int 2
##  $ ifault      : int 0
##  - attr(*, "class")= chr "kmeans"
```

]

.pull-left60[

- `cluster`: cluster ID for each observation
- `centers`: centroid positions


```r
df %&gt;%
* mutate(cluster = as.factor(clustering$cluster)) %&gt;%
  ggplot(aes(bill_length_mm, bill_depth_mm)) +
  geom_point(aes(color = cluster)) + 
* geom_point(data = as_tibble(clustering$centers),
*            pch = 18, size = 5) +
  guides(color = FALSE)
```

]

.pull-right40[

&lt;img src="slides-kmeans_files/figure-html/unnamed-chunk-12-1.png" width="368.503937007874" /&gt;

]

---

.font90[


```r
str(clustering)
```

```
## List of 9
##  $ cluster     : int [1:342] 2 2 2 2 2 2 2 2 2 2 ...
##  $ centers     : num [1:3, 1:2] 45.5 38.4 50.9 15.6 18.3 ...
##   ..- attr(*, "dimnames")=List of 2
##   .. ..$ : chr [1:3] "1" "2" "3"
##   .. ..$ : chr [1:2] "bill_length_mm" "bill_depth_mm"
*##  $ totss       : num 11494
*##  $ withinss    : num [1:3] 755 944 618
*##  $ tot.withinss: num 2317
*##  $ betweenss   : num 9177
*##  $ size        : int [1:3] 116 141 85
*##  $ iter        : int 2
##  $ ifault      : int 0
##  - attr(*, "class")= chr "kmeans"
```

]

- `totss`: total sum of squares (sum of squared distances to global mean over all observations)
- `withinss`: within sum of squares (sum of squared distances to cluster centroid)
- `tot.withinss`: sum of `withinss`
- `betweenss`: `totss` - `tot.withinss` (reduction in sum of squares)
- `size`: number of cluster members
- `iter`: number of iterations until convergence

---

class: exercise-blue
name: formative-assessment-2

## Your Turn (exercise.R)

#### Run K-means clustering on the Palmer penguins data with K=3.&lt;br&gt; Instead of initializing the centroids randomly, use the provided custom coordinates. &lt;br&gt;Return the number of observations for each cluster.


```r
library(dplyr)
library(tidyr)
library(palmerpenguins)

# Prepare 2-dimensional data for clustering
df &lt;- penguins %&gt;% 
  select(bill_length_mm, bill_depth_mm) %&gt;% 
  drop_na()

# Run K-means clustering using custom initial centroids
clustering &lt;- kmeans(_____, 
                     _____ = tibble(
                       bill_length_mm = c(40, 50, 60), 
                       bill_depth_mm = c(15, 18, 21)
                     )
)
clustering

# Return cluster sizes
clustering$_____
```

???

- df
- centers
- size

**faded example** vs. Parson's problem

---

name: learner-persona

## Learner Persona&amp;ast; 

&lt;!-- (**Course:** "Introduction to Data Mining with R") --&gt;

.footnote[

&amp;ast; This lesson on "K-Means Clustering in R" is meant to be a part of the (fictional) undergraduate course "Data Mining with R".

]

.pull-left70[

.font120[_Meet Nathalie..._]

**1. Background:** Nathalie is a 22-year-old undergraduate student pursuing a Bachelor's degree in "Operations Research and Business Analytics". 
She is born in Turkey, and moved to Germany to take up her study. 

**2. Relevant Experience:** Currently, Nathalie is in her 5th semester. 
Until now, she has been analyzing data mainly with Excel, and has some working knowledge of SPSS, but she has never programmed in R prior to taking this course. This is also her first course on data science.

**3. Needs:** 
Nathalie is enrolled in this course which is required for her degree, so she needs to pass it.
However, she is also interested in this course because she knows that showing data science and R proficiency on her CV will increase her value on the job market.
She needs a gentle introduction to these topics which doesn't require familiarity with R. 
&lt;!-- focusing on differences between the tools she currently uses and a "real" programming language (_What are loops?, What is a function?, ..._). --&gt;

**4. Special considerations:** Nathalie has little experience in programming and she doesn't have a very powerful laptop.

]

.pull-right30[

&lt;img src="images/woman-studying.jpg" width="100%" style="display: block; margin: auto;" /&gt;

.font80[

Photo: [Andrew Neel](https://unsplash.com/photos/ute2XAFQU2I)

]
]

???

Explain in _Their Terms_: This is a course that introduces you to the foundations of data science. We will learn the most important data science methods and we will apply them in small case studies using the R programming language. 

---

name: concept-map

## Concept Map

&lt;img src="images/concept-map.png" width="100%" style="display: block; margin: auto;" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
